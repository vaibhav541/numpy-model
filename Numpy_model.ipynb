{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialization of Weights and Creating a Random Dataset\n",
    "np.random.seed(1)\n",
    "A = np.random.randn(20, 32, 32,1)  #Dataset having 20 examples\n",
    "Y=np.random.randint(9,size=20)   #True Labels\n",
    "b = np.zeros((20,10))       #One-hot Encoding of True Labels\n",
    "b[np.arange(20), Y] = 1\n",
    "Y=b\n",
    "W1 = np.random.randn(3, 3,1, 4)\n",
    "W3 =np.random.randn(1024,10)\n",
    "\n",
    "b1 = np.random.randn(1, 1,1, 4)\n",
    "b3=np.random.randn(1,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b):\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = 2\n",
    "    \n",
    "    n_H = 16\n",
    "    n_W = 16\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        a_prev = A_prev[i]                     # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    if a_slice_prev.shape==W[...,c].shape:\n",
    "                        Z[i, h, w, c]=np.sum(np.multiply(a_slice_prev, W[...,c])+  b[...,c] )\n",
    "                         #We have to slice the weights for the last stride both verically and horizontally to prevent any error to occur as some part of it will not overlap the input matrix\n",
    "                    else:\n",
    "                        if a_slice_prev.shape==(3,2,1):     \n",
    "                            Z[i, h, w, c]=np.sum(np.multiply(a_slice_prev, W[:3,:2,:,c]) )#+  b[...,c])\n",
    "                        if a_slice_prev.shape==(2,3,1):\n",
    "                            Z[i, h, w, c]=np.sum(np.multiply(a_slice_prev, W[:2,:3,:,c]) +  b[...,c])\n",
    "                        \n",
    "                    \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \n",
    "    \n",
    "  \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b) = cache\n",
    "    \n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = 2\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev = A_prev[i]\n",
    "        da_prev = dA_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        \n",
    "        dA_prev[i, :, :, :] = da_prev\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "   \n",
    "    A = np.maximum(0,Z)  \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "   \n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, dZ should be also zero\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_forward(A, W, b):\n",
    "    Z = np.dot(A,W) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_backward(dZ,cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "\n",
    "    dW = np.dot( cache[0].T,dZ) / m\n",
    "    #print(dW.shape)\n",
    "    db = np.squeeze(np.sum(dZ, axis=0, keepdims=True)) / m\n",
    "    #print(db.shape)\n",
    "    dA_prev = np.dot( dZ,cache[1].T)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_model(A,Wc,Wf1,bc,bf1):\n",
    "    caches = []\n",
    "    Z,cache=conv_forward(A,Wc,bc)\n",
    "    #print(Z.shape)\n",
    "    Z=Z.reshape(20,-1)\n",
    "    caches.append(cache)\n",
    "    Z,cache=relu(Z)\n",
    "    #print(Z.shape)\n",
    "    caches.append(cache)\n",
    "    Z,cache=fc_forward(Z,Wf1,bf1)\n",
    "    #print(Z.shape)\n",
    "    caches.append(cache)\n",
    "    AL=softmax(Z)\n",
    "    output=np.argmax(AL,axis=1)\n",
    "    \n",
    "    return AL,caches,output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop_model(AL, Y, caches):\n",
    "    grads = {}  #Dictionary to store gradients\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    grads[\"dA3\"], grads[\"dW3\"], grads[\"db3\"] = fc_backward(dAL, caches[-1])\n",
    "    dZ2=relu_backward(grads[\"dA3\"],caches[-2])\n",
    "    dZ2=dZ2.reshape((20,16,16,4))\n",
    "    grads[\"dA1\"],grads[\"dW1\"],grads[\"db1\"]=conv_backward(dZ2,caches[-3])\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(W1,b1,W3,b3, grads, lr=0.1):\n",
    "    W1=W1-lr*grads[\"dW1\"]\n",
    "    b1=b1-lr*grads[\"db1\"]\n",
    "    W3=W3-lr*grads[\"dW3\"]\n",
    "    b3=b3-lr*grads[\"db3\"]\n",
    "    \n",
    "    \n",
    "        \n",
    "    return W1,b1,W3,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhav\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in maximum\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Vaibhav\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in less_equal\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(i)\n",
    "    AL,caches,output=forward_model(A,W1,W3,b1,b3)\n",
    "    grads=backprop_model(AL, Y, caches)\n",
    "    W1,b1,W3,b3=update_parameters(W1,b1,W3,b3, grads, 0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
